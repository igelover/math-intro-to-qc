{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dba718d",
   "metadata": {},
   "source": [
    "<a href=\"https://qworld.net\" target=\"_blank\" align=\"left\"><img src=\"../qworld/images/header.jpg\"  align=\"left\"></a>\n",
    "$$\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\inner}[2]{\\left\\langle#1,#2\\right\\rangle}\n",
    "\\newcommand{\\bra}[1]{\\left\\langle#1\\right|}\n",
    "\\newcommand{\\ket}[1]{\\left|#1\\right\\rangle}\n",
    "\\newcommand{\\braket}[2]{\\left\\langle#1|#2\\right\\rangle}\n",
    "\\newcommand{\\ketbra}[2]{\\left|#1\\right\\rangle\\left\\langle#2\\right|}\n",
    "\\newcommand{\\angleset}[1]{\\left\\langle#1\\right\\rangle}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff6a6e",
   "metadata": {},
   "source": [
    "# Spectral Theory\n",
    "\n",
    "_prepared by Israel Gelover_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63b5e2",
   "metadata": {},
   "source": [
    "### <a name=\"remark_2_17\">Remark 2.17</a> Spectral Theory\n",
    "\n",
    "Given $A:\\mathcal{H} \\to \\mathcal{H}$ a linear operator, we can consider $A_\\lambda = A - \\lambda I$ with $\\lambda \\in \\mathbb{C}$. The study of the distribution of the values of $\\lambda$ for which $A_\\lambda$ whether has or does not have an inverse is called _Spectral Theory_ of the operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b5133",
   "metadata": {},
   "source": [
    "In quantum computing we will only work with finite-dimensional operators and this type of operators only have one type of spectrum, which are the eigenvalues of the operator.\n",
    "\n",
    "Let us recall that to obtain the eigenvalues of a matrix $A$ we calculate $\\det(A - \\lambda I) = 0$. That is, we look for the values of $\\lambda$ for which the matrix $A - \\lambda I$ has no inverse. Meaning that the eigenvalues are a particular case of the spectrum of an operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff0949",
   "metadata": {},
   "source": [
    "### <a name=\"definition_2_18\">Definition 2.18</a> Eigenvalues and Eigenvectors\n",
    "\n",
    "Let $A:\\mathcal{H} \\to \\mathcal{H}$ be a linear operator\n",
    "\n",
    "1. The set $P(A) = \\set{\\lambda \\in \\mathbb{C} \\mid A\\ket{f} = \\lambda\\ket{f} \\text{ for some } \\ket{f} \\neq 0}$ is called the point spectrum of $A$, and its elements are known as _Eigenvalues_ of $A$. **Remark:** If $\\mathcal{H}$ is of finite dimension, then $P(A) = \\set{\\text{solutions of } \\det(A - \\lambda I) = 0}$\n",
    "2. Let $\\lambda_0 \\in P(A)$, if $A\\ket{f} = \\lambda_0 \\ket{f}$ has non-trivial solutions, that is, $\\lambda_0$ is an eigenvalue, we will denote by$\\mathcal{L}(\\lambda_0)$ the subspace of $\\mathcal{H}$ generated by the solutions\n",
    "of $A\\ket{f} = \\lambda_0 \\ket{f}$.\n",
    "3. $\\dim \\set{\\mathcal{L}(\\lambda_0)}$ is called the multiplicity of $\\lambda_0$. Let us recall that when finding the non-trivial solutions of $\\det(A - \\lambda I) = 0$ we can find that the monomial associated with an eigenvalue appears more than once in the characteristic polynomial of $A$, this number of repetitions is what we know as the multiplicity of the eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b1f866",
   "metadata": {},
   "source": [
    "Intuitively we can interpret that finding the eigenvalues of an operator has the geometric equivalence of finding subspaces that remain invariant under the application of the operator. For example, if we have a linear operator $A:\\mathbb{R}^2 \\to \\mathbb{R}^2$ and we find the eigenvalues of $A$, this would be equivalent to finding a line that is fixed with respect to the application of the operator $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9946b73",
   "metadata": {},
   "source": [
    "### Task 1.\n",
    "\n",
    "(On paper) Let $A:\\mathcal{H} \\to \\mathcal{H}$ be a self-adjoint operator, (i.e $A=A^\\dagger$).  \n",
    "If $A\\ket{f} = \\lambda\\ket{f}$, use <a href=\"./HilbertSpace.ipynb#definition_2_2\">Definition 2.2</a> and <a href=\"./LinearOperators.ipynb#definition_2_12\">Definition 2.12</a> to conclude that the eigenvalues of an autoadjoint operator are real numbers.\n",
    "\n",
    "<a href=\"./SpectralTheory_Solutions.ipynb#task_1\">Click here for solution</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e18d",
   "metadata": {},
   "source": [
    "### Task 2.\n",
    "\n",
    "(On paper) Let $U:\\mathcal{H} \\to \\mathcal{H}$ be a unitary operator.  \n",
    "If $U\\ket{f} = \\lambda\\ket{f}$, calculate $\\braket{f}{f}$ to conclude that the eigenvalues of a unit operator are complex numbers with norm one.\n",
    "\n",
    "<a href=\"./SpectralTheory_Solutions.ipynb#task_2\">Click here for solution</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879a596",
   "metadata": {},
   "source": [
    "### <a name=\"proposition_2_19\">Proposition 2.19</a>\n",
    "\n",
    "Let $A:\\mathcal{H} \\to \\mathcal{H}$ be a self-adjoint operator, then the eigenvectors corresponding to different eigenvalues are orthogonal.\n",
    "\n",
    "**Remark:** From this proposition the _Spectral Theorem_ can be derived, which states that there exists an orthonormal basis of the space consisting of eigenvectors of the self-adjoint operator.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $\\lambda_1, \\lambda_2$ be different eigenvalues, that is $\\lambda_1 \\neq \\lambda_2$, such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    A\\ket{f_1} = \\lambda_1\\ket{f_1} \\enspace \\text{ and } \\enspace A\\ket{f_2} = \\lambda_2\\ket{f_2}\n",
    "\\end{equation*}\n",
    "\n",
    "So, on the one hand we have\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\ket{f_1}\\cdot (A\\ket{f_2}) = \\ket{f_1} \\cdot (\\lambda_2\\ket{f_2}) = \\lambda_2(\\ket{f_1} \\cdot \\ket{f_2})\n",
    "\\end{equation*}\n",
    "\n",
    "On the other hand\n",
    "\n",
    "\\begin{equation*}\n",
    "    (A\\ket{f_1}) \\cdot \\ket{f_2} = (\\lambda_1\\ket{f_1}) \\cdot \\ket{f_2} = \\lambda_1^*(\\ket{f_1} \\cdot \\ket{f_2})\n",
    "\\end{equation*}\n",
    "\n",
    "Since $A$ is a self-adjoint operator, we know that $\\ket{f_1}\\cdot (A\\ket{f_2}) = (A\\ket{f_1}) \\cdot \\ket{f_2}$ and also $\\lambda_1, \\lambda_2 \\in \\mathbb{R}$, then $\\lambda_1\\braket{f_1}{f_2} = \\lambda_2\\braket{f_1}{f_2}$.\n",
    "\n",
    "Therefore $\\braket{f_1}{f_2} = 0$, since $\\lambda_1 \\neq \\lambda_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175e4e4",
   "metadata": {},
   "source": [
    "### <a name=\"definition_2_19\">Definition 2.19</a> Outer Product\n",
    "\n",
    "Let $\\ket{f}, \\ket{g} \\in \\mathcal{H}$ Hilbert space, the _Outer Product_ of $\\ket{f}$ and $\\ket{g}$ is a linear operator such that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\ketbra{f}{g}:\\mathcal{H} &\\to \\mathcal{H} \\\\\n",
    "        \\ket{x} &\\to \\ket{f}\\braket{g}{x}\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "That is, applying the outer product of $\\ket{f}$ and $\\ket{g}$ to an arbitrary vector $\\ket{x}$, is equivalent to calculate the inner product of $\\ket{g}$ with $\\ket{x}$ and multiplying the resulting scalar by $\\ket{f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1baf1",
   "metadata": {},
   "source": [
    "### <a name=\"examples\">Examples</a>\n",
    "\n",
    "1. Let $\\ket{\\psi} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})$ and $\\ket{\\phi} = \\ket{0}$, then\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\ketbra{\\psi}{\\phi} &= \\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1})\\bra{0} = \\frac{1}{\\sqrt{2}}(\\ketbra{0}{0}+\\ketbra{1}{0}) \\\\\n",
    "        &= \\frac{1}{\\sqrt{2}}\\left[\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix}\\right] \\\\\n",
    "        &= \\frac{1}{\\sqrt{2}}\\left[\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}\\right] = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix}\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "Let us recall that **Definition 2.20** tells us that the outer product of two vectors is a linear operator, in this case we have the outer product of two qubits and therefore we obtain a two-dimensional operator, that is, we obtain a $2 \\times 2$ matrix. Note that from the expression of the outer product using Dirac notation we can deduce how it looks like in matrix notation, as follows:\n",
    "\n",
    "\\begin{equation}\\label{dirac_matriz}\n",
    "    \\ketbra{\\psi}{\\phi} = \\frac{1}{\\sqrt{2}}(\\overbrace{\\ket{0}}^\\text{row} \\underbrace{\\bra{0}}_\\text{column} + \\overbrace{\\ket{1}}^\\text{row} \\underbrace{\\bra{0}}_\\text{column})\n",
    "\\end{equation}\n",
    "\n",
    "This tells us that the associated matrix will have $\\frac{1}{\\sqrt{2}}$ in row $0$ - column $0$ and in row $1$ - column $0$ and zeros in the rest of the positions, the same as we obtained when calculating using vector notation:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\ketbra{\\psi}{\\phi} = \\frac{1}{\\sqrt{2}}(\\ketbra{0}+\\ketbra{1}{0}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "2. Let $\\ket{\\psi}, \\ket{\\phi}$ be as in the previous example, then\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        (\\ketbra{\\psi}{\\phi})\\ket{1} = \\ket{\\psi}\\braket{\\phi}{1} &= \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\braket{0}{1} = 0 \\\\\n",
    "        &= \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 0 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "In both cases we obtain vector **0**, but clearly, Dirac notation is much more practical and therefore we will prefer it in the rest of the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33054d9",
   "metadata": {},
   "source": [
    "### <a name=\"definition_2_21\">Definition 2.21</a> Completeness Relation\n",
    "\n",
    "Let $\\set{\\ket{u_i}}_{i=1}^{n}$ be an orthonormal basis of a space $\\mathcal{H} \\implies \\sum_{i=1}^{n}\\ketbra{u_i}{u_i} = I$\n",
    "\n",
    "Let us recall that the outer product of two vectors is a linear operator, and since the sum of linear operators is again a linear operator, in this case we obtain the identity operator.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Let $\\ket{\\psi} \\in \\mathcal{H}$ be a vector, we can express it in terms of the basis as $\\ket{\\psi} = \\sum_{j=1}^{n}c_j\\ket{u_j}$, with $c_j \\in \\mathbb{F}$ elements of the field. Thus\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "        \\left(\\sum_{i=1}^{n}\\ketbra{u_i}{u_i}\\right)\\ket{\\psi} &= \\sum_{i=1}^{n}\\ket{u_i}\\braket{u_i}{\\psi} \\\\\n",
    "        &= \\sum_{i=1}^{n}\\ket{u_i} \\sum_{j=1}^{n}c_j \\braket{u_i}{u_j} = \\sum_{i=1}^{n}c_i\\ket{u_i} = \\ket{\\psi}\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "since $\\braket{u_i}{u_j}$ takes value $1$ when $i=j$ and $0$ when $i \\neq j$, as they are elements of an orthonormal basis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
